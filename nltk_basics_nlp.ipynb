{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XFh4BRFVCg9M",
        "D-sTCTe2CqsM",
        "5KKhB7ah8gPY"
      ],
      "authorship_tag": "ABX9TyN3AvVGNjZg5vnPdAq2J/rg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4entertainment/nlp_works/blob/main/nltk_basics_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "XFh4BRFVCg9M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xbTw_vMJ7KcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c23522-932b-408c-d3cb-8b453d2bff79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# sent_tokenize for sentence tokenization\n",
        "# word_tokenize for word tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Alan Turing, in full Alan Mathison Turing, (born June 23, 1912, London, England—died June 7, 1954, Wilmslow, Cheshire), British mathematician and logician who made major contributions to mathematics, cryptanalysis, logic, philosophy, and mathematical biology and also to the new areas later named computer science, cognitive science, artificial intelligence, and artificial life.\""
      ],
      "metadata": {
        "id": "-eTyWN4_G1ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.split()\n",
        "# The split() method splits a string into a list."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpaABcaxIBIW",
        "outputId": "06b1fb60-6f8b-4f7c-b81b-985d11da0aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Alan',\n",
              " 'Turing,',\n",
              " 'in',\n",
              " 'full',\n",
              " 'Alan',\n",
              " 'Mathison',\n",
              " 'Turing,',\n",
              " '(born',\n",
              " 'June',\n",
              " '23,',\n",
              " '1912,',\n",
              " 'London,',\n",
              " 'England—died',\n",
              " 'June',\n",
              " '7,',\n",
              " '1954,',\n",
              " 'Wilmslow,',\n",
              " 'Cheshire),',\n",
              " 'British',\n",
              " 'mathematician',\n",
              " 'and',\n",
              " 'logician',\n",
              " 'who',\n",
              " 'made',\n",
              " 'major',\n",
              " 'contributions',\n",
              " 'to',\n",
              " 'mathematics,',\n",
              " 'cryptanalysis,',\n",
              " 'logic,',\n",
              " 'philosophy,',\n",
              " 'and',\n",
              " 'mathematical',\n",
              " 'biology',\n",
              " 'and',\n",
              " 'also',\n",
              " 'to',\n",
              " 'the',\n",
              " 'new',\n",
              " 'areas',\n",
              " 'later',\n",
              " 'named',\n",
              " 'computer',\n",
              " 'science,',\n",
              " 'cognitive',\n",
              " 'science,',\n",
              " 'artificial',\n",
              " 'intelligence,',\n",
              " 'and',\n",
              " 'artificial',\n",
              " 'life.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(text)\n",
        "# word_tokenize for word tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYyIMAksIOnO",
        "outputId": "235371fc-35d3-4fa6-8ffe-97b49afc9447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Alan',\n",
              " 'Turing',\n",
              " ',',\n",
              " 'in',\n",
              " 'full',\n",
              " 'Alan',\n",
              " 'Mathison',\n",
              " 'Turing',\n",
              " ',',\n",
              " '(',\n",
              " 'born',\n",
              " 'June',\n",
              " '23',\n",
              " ',',\n",
              " '1912',\n",
              " ',',\n",
              " 'London',\n",
              " ',',\n",
              " 'England—died',\n",
              " 'June',\n",
              " '7',\n",
              " ',',\n",
              " '1954',\n",
              " ',',\n",
              " 'Wilmslow',\n",
              " ',',\n",
              " 'Cheshire',\n",
              " ')',\n",
              " ',',\n",
              " 'British',\n",
              " 'mathematician',\n",
              " 'and',\n",
              " 'logician',\n",
              " 'who',\n",
              " 'made',\n",
              " 'major',\n",
              " 'contributions',\n",
              " 'to',\n",
              " 'mathematics',\n",
              " ',',\n",
              " 'cryptanalysis',\n",
              " ',',\n",
              " 'logic',\n",
              " ',',\n",
              " 'philosophy',\n",
              " ',',\n",
              " 'and',\n",
              " 'mathematical',\n",
              " 'biology',\n",
              " 'and',\n",
              " 'also',\n",
              " 'to',\n",
              " 'the',\n",
              " 'new',\n",
              " 'areas',\n",
              " 'later',\n",
              " 'named',\n",
              " 'computer',\n",
              " 'science',\n",
              " ',',\n",
              " 'cognitive',\n",
              " 'science',\n",
              " ',',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " ',',\n",
              " 'and',\n",
              " 'artificial',\n",
              " 'life',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(text)\n",
        "# sent_tokenize for sentence tokenization"
      ],
      "metadata": {
        "id": "VkwCBXYSId9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508d4027-41c4-4a32-d58c-cd3a7b71d3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Alan Turing, in full Alan Mathison Turing, (born June 23, 1912, London, England—died June 7, 1954, Wilmslow, Cheshire), British mathematician and logician who made major contributions to mathematics, cryptanalysis, logic, philosophy, and mathematical biology and also to the new areas later named computer science, cognitive science, artificial intelligence, and artificial life.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in word_tokenize(text):\n",
        "  print(token)\n",
        "# printing the tokens in word_tokenized text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqV1YctqOOp0",
        "outputId": "5f4c545c-8e1a-4d6d-aa79-593f11cac5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alan\n",
            "Turing\n",
            ",\n",
            "in\n",
            "full\n",
            "Alan\n",
            "Mathison\n",
            "Turing\n",
            ",\n",
            "(\n",
            "born\n",
            "June\n",
            "23\n",
            ",\n",
            "1912\n",
            ",\n",
            "London\n",
            ",\n",
            "England—died\n",
            "June\n",
            "7\n",
            ",\n",
            "1954\n",
            ",\n",
            "Wilmslow\n",
            ",\n",
            "Cheshire\n",
            ")\n",
            ",\n",
            "British\n",
            "mathematician\n",
            "and\n",
            "logician\n",
            "who\n",
            "made\n",
            "major\n",
            "contributions\n",
            "to\n",
            "mathematics\n",
            ",\n",
            "cryptanalysis\n",
            ",\n",
            "logic\n",
            ",\n",
            "philosophy\n",
            ",\n",
            "and\n",
            "mathematical\n",
            "biology\n",
            "and\n",
            "also\n",
            "to\n",
            "the\n",
            "new\n",
            "areas\n",
            "later\n",
            "named\n",
            "computer\n",
            "science\n",
            ",\n",
            "cognitive\n",
            "science\n",
            ",\n",
            "artificial\n",
            "intelligence\n",
            ",\n",
            "and\n",
            "artificial\n",
            "life\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Words"
      ],
      "metadata": {
        "id": "D-sTCTe2CqsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# library importations"
      ],
      "metadata": {
        "id": "A7A2eNjVCv1o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb29fbe7-bdf3-4dc7-a038-153f4f359c93"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text for processing\n",
        "text = \"Fazıl Say was born in 1970. His father, Ahmet Say was an author and musicologist. His mother, Gürgün Say was a pharmacist. His grandfather Fazıl Say with whom he shares the same name with was a member of the Spartakusbund.[1] Say was a child prodigy, who was able to do basic arithmetic with 4-digit numbers at the age of two. His father, having found out that he was playing the melody of Daha Dün Annemizin (Turkish version of Ah! vous dirai-je, maman) on a makeshift flute with no prior training, enlisted the help of Ali Kemal Kaya, an oboist and family friend. At the age of three, Say started his piano lessons under the tutelage of pianist Mithat Fenmen\""
      ],
      "metadata": {
        "id": "GC0PueDIz5Co"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at the stopwords in english\n",
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA0JhBGv0Nil",
        "outputId": "a65ab496-66e5-484d-c746-66d26987a4ef"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at the stopwords in turkish\n",
        "stopwords.words('turkish')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLG_3jYF0dWt",
        "outputId": "b3ff78cb-df4d-49d4-f260-6a1c89250081"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['acaba',\n",
              " 'ama',\n",
              " 'aslında',\n",
              " 'az',\n",
              " 'bazı',\n",
              " 'belki',\n",
              " 'biri',\n",
              " 'birkaç',\n",
              " 'birşey',\n",
              " 'biz',\n",
              " 'bu',\n",
              " 'çok',\n",
              " 'çünkü',\n",
              " 'da',\n",
              " 'daha',\n",
              " 'de',\n",
              " 'defa',\n",
              " 'diye',\n",
              " 'eğer',\n",
              " 'en',\n",
              " 'gibi',\n",
              " 'hem',\n",
              " 'hep',\n",
              " 'hepsi',\n",
              " 'her',\n",
              " 'hiç',\n",
              " 'için',\n",
              " 'ile',\n",
              " 'ise',\n",
              " 'kez',\n",
              " 'ki',\n",
              " 'kim',\n",
              " 'mı',\n",
              " 'mu',\n",
              " 'mü',\n",
              " 'nasıl',\n",
              " 'ne',\n",
              " 'neden',\n",
              " 'nerde',\n",
              " 'nerede',\n",
              " 'nereye',\n",
              " 'niçin',\n",
              " 'niye',\n",
              " 'o',\n",
              " 'sanki',\n",
              " 'şey',\n",
              " 'siz',\n",
              " 'şu',\n",
              " 'tüm',\n",
              " 've',\n",
              " 'veya',\n",
              " 'ya',\n",
              " 'yani']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# choosing the stopwords as english. define as \"stopwords\" keyword\n",
        "stopwords = stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "f_nmfAi70y3y",
        "outputId": "21b94a19-0611-4954-b70c-bcf8dd30e271"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-3d6d7ec8f480>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# choosing the stopwords as english. define as \"stopwords\" keyword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "words = word_tokenize(text)"
      ],
      "metadata": {
        "id": "JNe9W5kN071c"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_words = []\n",
        "for word in words:\n",
        "  if word not in stopwords:\n",
        "      filtered_words.append(word)"
      ],
      "metadata": {
        "id": "9rGWg2891C8t"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLM95RnP4HaO",
        "outputId": "a1f05c06-d14c-493d-b284-b880a7bd5792"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Fazıl',\n",
              " 'Say',\n",
              " 'born',\n",
              " '1970',\n",
              " '.',\n",
              " 'His',\n",
              " 'father',\n",
              " ',',\n",
              " 'Ahmet',\n",
              " 'Say',\n",
              " 'author',\n",
              " 'musicologist',\n",
              " '.',\n",
              " 'His',\n",
              " 'mother',\n",
              " ',',\n",
              " 'Gürgün',\n",
              " 'Say',\n",
              " 'pharmacist',\n",
              " '.',\n",
              " 'His',\n",
              " 'grandfather',\n",
              " 'Fazıl',\n",
              " 'Say',\n",
              " 'shares',\n",
              " 'name',\n",
              " 'member',\n",
              " 'Spartakusbund',\n",
              " '.',\n",
              " '[',\n",
              " '1',\n",
              " ']',\n",
              " 'Say',\n",
              " 'child',\n",
              " 'prodigy',\n",
              " ',',\n",
              " 'able',\n",
              " 'basic',\n",
              " 'arithmetic',\n",
              " '4-digit',\n",
              " 'numbers',\n",
              " 'age',\n",
              " 'two',\n",
              " '.',\n",
              " 'His',\n",
              " 'father',\n",
              " ',',\n",
              " 'found',\n",
              " 'playing',\n",
              " 'melody',\n",
              " 'Daha',\n",
              " 'Dün',\n",
              " 'Annemizin',\n",
              " '(',\n",
              " 'Turkish',\n",
              " 'version',\n",
              " 'Ah',\n",
              " '!',\n",
              " 'vous',\n",
              " 'dirai-je',\n",
              " ',',\n",
              " 'maman',\n",
              " ')',\n",
              " 'makeshift',\n",
              " 'flute',\n",
              " 'prior',\n",
              " 'training',\n",
              " ',',\n",
              " 'enlisted',\n",
              " 'help',\n",
              " 'Ali',\n",
              " 'Kemal',\n",
              " 'Kaya',\n",
              " ',',\n",
              " 'oboist',\n",
              " 'family',\n",
              " 'friend',\n",
              " '.',\n",
              " 'At',\n",
              " 'age',\n",
              " 'three',\n",
              " ',',\n",
              " 'Say',\n",
              " 'started',\n",
              " 'piano',\n",
              " 'lessons',\n",
              " 'tutelage',\n",
              " 'pianist',\n",
              " 'Mithat',\n",
              " 'Fenmen']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IjmMeWqn4JfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "sRb8i7w54e0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# libraries for stemming"
      ],
      "metadata": {
        "id": "WNtO3gza4hXl"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PorterStemmer function for stem the words\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "nkXXPr6l4sX1"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of words called \"words\" for stemming\n",
        "words = ['drive', 'driving','driver','drives','drove','cats','children']"
      ],
      "metadata": {
        "id": "W7OkCHDR4yWV"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stem the words in \"words\" list and adding them to \"stemmedwords\" list\n",
        "stemmedwords = []\n",
        "for w in words:\n",
        "  stemmed_word = ps.stem(w)\n",
        "  stemmedwords.append(stemmed_word)\n"
      ],
      "metadata": {
        "id": "Aor_9kWa49X9"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing results\n",
        "print(\"original words:\", words)\n",
        "print(\"stems of words:\", stemmedwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcDZ2XeE5zAF",
        "outputId": "3ffc7276-f1b5-421d-a759-8a6b45993031"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original words: ['drive', 'driving', 'driver', 'drives', 'drove', 'cats', 'children']\n",
            "stems of words: ['drive', 'drive', 'driver', 'drive', 'drove', 'cat', 'children']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing results\n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fM2Nfam6O61",
        "outputId": "679ca939-2b4e-46c9-a5de-3e910c024158"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  :  drive\n",
            "driving  :  drive\n",
            "driver  :  driver\n",
            "drives  :  drive\n",
            "drove  :  drove\n",
            "cats  :  cat\n",
            "children  :  children\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pBOwFoBh6Yat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PoST"
      ],
      "metadata": {
        "id": "5KKhB7ah8gPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKmhReTH8jI4",
        "outputId": "fd332202-d217-4027-a4cb-7a3146edf1bb"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = \"The goal of this project was to familiarize myself with the inner workings of LSTMs using TensorFlow. The idea for using Nietzshe came from Fast.ai. Jeremey Howard uses this text to demonstrate how recurrent neural networks and LSTMs work. I enjoy Nietzsche's philosophies so I was curious to see what kind of blunt, existential truths the computer could generate. \""
      ],
      "metadata": {
        "id": "Sh3VCpYM9Cmt"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = nltk.word_tokenize(text_1)"
      ],
      "metadata": {
        "id": "E2ZEavqb9fKl"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8kpTDdF-KWF",
        "outputId": "8462defa-741d-4093-a51c-7a52a2c97bd3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'goal',\n",
              " 'of',\n",
              " 'this',\n",
              " 'project',\n",
              " 'was',\n",
              " 'to',\n",
              " 'familiarize',\n",
              " 'myself',\n",
              " 'with',\n",
              " 'the',\n",
              " 'inner',\n",
              " 'workings',\n",
              " 'of',\n",
              " 'LSTMs',\n",
              " 'using',\n",
              " 'TensorFlow',\n",
              " '.',\n",
              " 'The',\n",
              " 'idea',\n",
              " 'for',\n",
              " 'using',\n",
              " 'Nietzshe',\n",
              " 'came',\n",
              " 'from',\n",
              " 'Fast.ai',\n",
              " '.',\n",
              " 'Jeremey',\n",
              " 'Howard',\n",
              " 'uses',\n",
              " 'this',\n",
              " 'text',\n",
              " 'to',\n",
              " 'demonstrate',\n",
              " 'how',\n",
              " 'recurrent',\n",
              " 'neural',\n",
              " 'networks',\n",
              " 'and',\n",
              " 'LSTMs',\n",
              " 'work',\n",
              " '.',\n",
              " 'I',\n",
              " 'enjoy',\n",
              " 'Nietzsche',\n",
              " \"'s\",\n",
              " 'philosophies',\n",
              " 'so',\n",
              " 'I',\n",
              " 'was',\n",
              " 'curious',\n",
              " 'to',\n",
              " 'see',\n",
              " 'what',\n",
              " 'kind',\n",
              " 'of',\n",
              " 'blunt',\n",
              " ',',\n",
              " 'existential',\n",
              " 'truths',\n",
              " 'the',\n",
              " 'computer',\n",
              " 'could',\n",
              " 'generate',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6_p6xJz-Ln1",
        "outputId": "e6d17c31-ae05-4c84-ab67-2ec985ade8f5"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('goal', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('this', 'DT'),\n",
              " ('project', 'NN'),\n",
              " ('was', 'VBD'),\n",
              " ('to', 'TO'),\n",
              " ('familiarize', 'VB'),\n",
              " ('myself', 'PRP'),\n",
              " ('with', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('inner', 'JJ'),\n",
              " ('workings', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('LSTMs', 'NNP'),\n",
              " ('using', 'VBG'),\n",
              " ('TensorFlow', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('The', 'DT'),\n",
              " ('idea', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('using', 'VBG'),\n",
              " ('Nietzshe', 'NNP'),\n",
              " ('came', 'VBD'),\n",
              " ('from', 'IN'),\n",
              " ('Fast.ai', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Jeremey', 'NNP'),\n",
              " ('Howard', 'NNP'),\n",
              " ('uses', 'VBZ'),\n",
              " ('this', 'DT'),\n",
              " ('text', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('demonstrate', 'VB'),\n",
              " ('how', 'WRB'),\n",
              " ('recurrent', 'JJ'),\n",
              " ('neural', 'JJ'),\n",
              " ('networks', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('LSTMs', 'NNP'),\n",
              " ('work', 'NN'),\n",
              " ('.', '.'),\n",
              " ('I', 'PRP'),\n",
              " ('enjoy', 'VBP'),\n",
              " ('Nietzsche', 'NNP'),\n",
              " (\"'s\", 'POS'),\n",
              " ('philosophies', 'NNS'),\n",
              " ('so', 'IN'),\n",
              " ('I', 'PRP'),\n",
              " ('was', 'VBD'),\n",
              " ('curious', 'JJ'),\n",
              " ('to', 'TO'),\n",
              " ('see', 'VB'),\n",
              " ('what', 'WP'),\n",
              " ('kind', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('blunt', 'NN'),\n",
              " (',', ','),\n",
              " ('existential', 'JJ'),\n",
              " ('truths', 'NNS'),\n",
              " ('the', 'DT'),\n",
              " ('computer', 'NN'),\n",
              " ('could', 'MD'),\n",
              " ('generate', 'VB'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Name Entity Recognition"
      ],
      "metadata": {
        "id": "xVHtobLY-clm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "\n",
        "nltk.download('words')\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "# importing libraries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGS3m5LhAy4p",
        "outputId": "09007d24-be95-45b4-8ef3-111989e29e95"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### CREATE VIRTUAL DISPLAY ###\n",
        "!apt-get install -y xvfb # Install X Virtual Frame Buffer\n",
        "import os\n",
        "os.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
        "os.environ['DISPLAY']=':1.0'    # tell X clients to use our virtual DISPLAY :1.0."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TbLVqP9B7Hi",
        "outputId": "674956af-251f-4f9c-a301-dde67ebea07e"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 7,812 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.1 [28.0 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.1 [863 kB]\n",
            "Fetched 7,812 kB in 2s (4,451 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 120500 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.1_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.1_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_eng = \"William Bradley Pitt (born December 18, 1963) is an American actor and film producer. He is the recipient of various accolades, including two Academy Awards, two British Academy Film Awards, two Golden Globe Awards, and a Primetime Emmy Award. As a public figure, Pitt has been cited as one of the most powerful and influential people in the American entertainment industry. \""
      ],
      "metadata": {
        "id": "eGO2DBY9-jEe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tr = \"William Bradley Pitt (18 Aralık 1963; Shawnee, Oklahoma, ABD), Amerikalı oyuncu ve film yapımcısıdır. Yapım şirketi Plan B Entertainment ile yapımcı olarak bir Akademi Ödülü ve Primetime Emmy Ödülü'nün yanı sıra oyunculuğuyla iki Altın Küre Ödülü ve bir Akademi Ödülü de dâhil olmak üzere birçok ödül almaya hak kazanmıştır. \""
      ],
      "metadata": {
        "id": "7s_xqJa8_cGd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_engtext = nltk.word_tokenize(text_eng)\n",
        "tokenized_trtext = nltk.word_tokenize(text_tr)\n",
        "# tokenization"
      ],
      "metadata": {
        "id": "A2XkUeji_lw5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_eng = nltk.pos_tag(tokenized_engtext)\n",
        "tagged_tr = nltk.pos_tag(tokenized_trtext)"
      ],
      "metadata": {
        "id": "Hg7_DZZn_zJG"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "named_ent_eng = nltk.ne_chunk(tagged_eng)\n",
        "named_ent_tr = nltk.ne_chunk(tagged_tr)"
      ],
      "metadata": {
        "id": "0S4-iJZZAPYt"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "named_ent_eng.draw()\n",
        "# ???"
      ],
      "metadata": {
        "id": "3FuUnrI6AcKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "named_ent_tr.draw()\n",
        "# ???"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "-vyt5EDwBA7W",
        "outputId": "32a329f1-32f1-4567-dcaa-6c86350ca221"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5e0e53880a62>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnamed_ent_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'named_ent_tr' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A4ADbrykBUFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatizing"
      ],
      "metadata": {
        "id": "Ph6ovZB2COS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Difference between lemmatizing and stemming:**\n",
        "\n",
        "Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma."
      ],
      "metadata": {
        "id": "NVGxExTRCfPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4c07ZhnCZV3",
        "outputId": "9a46b0e1-c44a-42dc-ccc6-1993e58ed8b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "Quse5jAwC-Nf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of words called \"words\" for stemming\n",
        "words = ['drive', 'driving','driver','drives','drove','cats','children']"
      ],
      "metadata": {
        "id": "TiDDMKTsDG5e"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stem the words in \"words\" list and adding them to \"stemmedwords\" list\n",
        "lemmedwords = []\n",
        "for w in words:\n",
        "  lemmed_word = lem.lemmatize(w)\n",
        "  lemmedwords.append(lemmed_word)"
      ],
      "metadata": {
        "id": "_1keeNbADPEy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing results\n",
        "print(\"original words:\", words)\n",
        "print(\"lemmatized version of words:\", lemmedwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ANXAUMVEaDn",
        "outputId": "4a12d86a-d92c-48cd-8389-185491f41850"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original words: ['drive', 'driving', 'driver', 'drives', 'drove', 'cats', 'children']\n",
            "lemmatized version of words: ['drive', 'driving', 'driver', 'drive', 'drove', 'cat', 'child']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing results\n",
        "for w in words:\n",
        "    print(w, \" : \", lem.lemmatize(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0bujj0IDHuf",
        "outputId": "dacc30b7-5c77-4412-c6b8-b05f3bcd3697"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  :  drive\n",
            "driving  :  driving\n",
            "driver  :  driver\n",
            "drives  :  drive\n",
            "drove  :  drove\n",
            "cats  :  cat\n",
            "children  :  child\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lem.lemmatize('driving'))\n",
        "print(lem.lemmatize('driving','v'))\n",
        "\n",
        "print(lem.lemmatize('drove'))\n",
        "print(lem.lemmatize('driving','v'))\n",
        "\n",
        "# The lemma function takes the word as a noun. We denote it as a verb with the 'v' expression and we can go to its root."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km1w_hvfDd_3",
        "outputId": "2f208be1-cab7-4f46-b81d-92d5789f7f0a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "driving\n",
            "drive\n",
            "drove\n",
            "drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0RD7fFC8Fh3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BoAATkP6GIu6"
      }
    }
  ]
}